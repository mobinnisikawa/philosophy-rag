import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from pypdf import PdfReader
import os
import json

# ===== ãƒšãƒ¼ã‚¸è¨­å®š =====
st.set_page_config(page_title="å“²å­¦RAG", layout="wide")
st.title("ğŸ“š å“²å­¦RAGã‚·ã‚¹ãƒ†ãƒ  (Streamlitç‰ˆ)")

# ===== å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ« =====
HISTORY_FILE = "chat_history.json"

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, list):  # ã“ã“ã§å¿…ãšãƒªã‚¹ãƒˆã‹ç¢ºèª
                return data
            else:
                return []
        except:
            return []
    return []

def save_history(history):
    if not isinstance(history, list):
        history = []
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

# ===== PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
uploaded_files = st.file_uploader(
    "PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
    type=["pdf"], accept_multiple_files=True
)

all_docs = []
if uploaded_files:
    for uploaded_file in uploaded_files:
        reader = PdfReader(uploaded_file)
        text = "".join(p.extract_text() or "" for p in reader.pages)
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = [Document(page_content=chunk, metadata={"source": uploaded_file.name})
                for chunk in splitter.split_text(text)]
        all_docs.extend(docs)

    st.success(f"âœ… å…¨PDFã®åˆè¨ˆãƒãƒ£ãƒ³ã‚¯æ•°: {len(all_docs)}")

    # ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆ
    emb = OpenAIEmbeddings(model="text-embedding-3-small")
    vs = FAISS.from_documents(all_docs, emb)
    retriever = vs.as_retriever(search_type="mmr", search_kwargs={"k": 8, "fetch_k": 20})

    # ===== è³ªå•å…¥åŠ› =====
    query = st.text_input("â“ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if query:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        docs = retriever.get_relevant_documents(query)
        context = "\n".join([d.page_content for d in docs])

        prompt = f"æ¬¡ã®è³‡æ–™ã‚’ä½¿ã£ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\nè³‡æ–™:\n{context}\n\nè³ªå•: {query}\n\nç­”ãˆ:"
        answer = llm.predict(prompt)

        st.subheader("ğŸ’¡ å›ç­”")
        st.write(answer)

        # å±¥æ­´ä¿å­˜
        history = load_history()
        if not isinstance(history, list):
            history = []
        history.append({"question": query, "answer": answer})
        save_history(history)

        # å‚è€ƒæ–‡çŒ®è¡¨ç¤º
        with st.expander("ğŸ“‘ å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"):
            for d in docs:
                st.markdown(f"- **{d.metadata['source']}**: {d.page_content[:200]}...")

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å±¥æ­´è¡¨ç¤º =====
with st.sidebar:
    st.header("ğŸ•‘ ãƒãƒ£ãƒƒãƒˆå±¥æ­´")

    history = load_history()
    if history:
        st.write("æœ€æ–°5ä»¶ï¼ˆä¸‹ã«å…¨ä»¶è¡¨ç¤º & ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯ï¼‰")
        for qa in history[-5:]:
            st.markdown(f"**Q:** {qa['question']}\n\n**A:** {qa['answer'][:100]}...")

        # å…¨ä»¶è¡¨ç¤º
        if st.checkbox("å…¨å±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹"):
            for i, qa in enumerate(history, 1):
                st.markdown(f"{i}. **Q:** {qa['question']}\n\nã€€**A:** {qa['answer']}")

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.download_button(
            label="ğŸ“¥ å±¥æ­´ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (JSON)",
            data=json.dumps(history, ensure_ascii=False, indent=2),
            file_name="chat_history.json",
            mime="application/json"
        )
    else:
        st.info("ã¾ã å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è³ªå•ã™ã‚‹ã¨ã“ã“ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚")
