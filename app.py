import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from pypdf import PdfReader
import os
import json
import datetime

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="å“²å­¦RAG", layout="wide")
st.title("ğŸ“š å“²å­¦RAGã‚·ã‚¹ãƒ†ãƒ  (Streamlitç‰ˆ)")

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜é–¢æ•° ---
def save_chat(query, answer):
    log = {
        "time": str(datetime.datetime.now()),
        "query": query,
        "answer": answer
    }
    with open("chat_history.json", "a", encoding="utf-8") as f:
        f.write(json.dumps(log, ensure_ascii=False) + "\n")

# --- PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
uploaded_files = st.file_uploader(
    "PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
    type=["pdf"],
    accept_multiple_files=True
)

all_docs = []
if uploaded_files:
    for uploaded_file in uploaded_files:
        reader = PdfReader(uploaded_file)
        text = "".join(p.extract_text() or "" for p in reader.pages)
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = [
            Document(page_content=chunk, metadata={"source": uploaded_file.name})
            for chunk in splitter.split_text(text)
        ]
        all_docs.extend(docs)

    st.success(f"âœ… å…¨PDFã®åˆè¨ˆãƒãƒ£ãƒ³ã‚¯æ•°: {len(all_docs)}")

    # --- ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆ ---
    emb = OpenAIEmbeddings(model="text-embedding-3-small")
    vs = FAISS.from_documents(all_docs, emb)
    retriever = vs.as_retriever(search_type="mmr", search_kwargs={"k": 8, "fetch_k": 20})

    # --- QA ã‚¨ãƒªã‚¢ ---
    query = st.text_input("â“ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if query:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        docs = retriever.get_relevant_documents(query)
        context = "\n".join([d.page_content for d in docs])

        prompt = f"æ¬¡ã®è³‡æ–™ã‚’è¸ã¾ãˆã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\nè³‡æ–™:\n{context}\n\nè³ªå•: {query}\n\nç­”ãˆ:"
        answer = llm.predict(prompt)

        st.subheader("ğŸ’¬ å›ç­”")
        st.write(answer)

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
        save_chat(query, answer)

        # å‚ç…§ã—ãŸæ–‡æ›¸ã‚‚è¡¨ç¤º
        with st.expander("ğŸ” å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"):
            for d in docs:
                st.markdown(f"**{d.metadata['source']}**: {d.page_content[:200]}...")

else:
    st.info("å·¦ã®ãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å±¥æ­´è¡¨ç¤º ---
st.sidebar.title("ğŸ“œ ãƒãƒ£ãƒƒãƒˆå±¥æ­´")
try:
    with open("chat_history.json", "r", encoding="utf-8") as f:
        lines = f.readlines()
        for line in lines[-5:]:  # ç›´è¿‘5ä»¶ã ã‘è¡¨ç¤º
            item = json.loads(line)
            st.sidebar.markdown(
                f"**Q:** {item['query']}  \n**A:** {item['answer']}  \nğŸ•’ {item['time']}"
            )
except FileNotFoundError:
    st.sidebar.write("ã¾ã å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
